# agentic-ai-topic-master-paper
A multi-agent orchestration system built with **Python**, **LangGraph**, **LangChain**, and **FastAPI**. This project implements a **Topic Master Architecture**, a nested graph topology that dynamically routes user intent to specialized expert agents (such as Diagnosis and Appointment) while maintaining complex conversational state.

## Repository Structure
```
Repository Structure (src):
â”œâ”€â”€ __init__.py
â”œâ”€â”€ agentic_network
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent_graph.py
â”‚   â”œâ”€â”€ agents
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent_data.py
â”‚   â”‚   â”œâ”€â”€ appointment
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ appointment_agent.py
â”‚   â”‚   â”‚   â””â”€â”€ system_prompt.py
â”‚   â”‚   â”œâ”€â”€ diagnosis
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ diagnosis_agent.py
â”‚   â”‚   â”‚   â””â”€â”€ system_prompt.py
â”‚   â”‚   â”œâ”€â”€ post_processing_agent.py
â”‚   â”‚   â”œâ”€â”€ pre_processing_agent.py
â”‚   â”‚   â”œâ”€â”€ tools_agent.py
â”‚   â”‚   â””â”€â”€ topic_manager_cluster
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ agents
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ new_topic_agent.py
â”‚   â”‚       â”‚   â”œâ”€â”€ post_processing_agent.py
â”‚   â”‚       â”‚   â”œâ”€â”€ pre_processing_agent.py
â”‚   â”‚       â”‚   â”œâ”€â”€ previous_topics_checker_agent.py
â”‚   â”‚       â”‚   â”œâ”€â”€ router_agent.py
â”‚   â”‚       â”‚   â””â”€â”€ topic_change_checker_agent.py
â”‚   â”‚       â”œâ”€â”€ core
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ topic_manager_routes.py
â”‚   â”‚       â”‚   â””â”€â”€ topic_manager_state.py
â”‚   â”‚       â”œâ”€â”€ routing
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ decide_topic_selected.py
â”‚   â”‚       â”œâ”€â”€ topic_manager_cluster.py
â”‚   â”‚       â””â”€â”€ utils
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â””â”€â”€ topic_manager_util.py
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent_state.py
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â”œâ”€â”€ routing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tools_condition.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_agent.py
â”‚       â”œâ”€â”€ base_utils.py
â”‚       â”œâ”€â”€ custom_react_agent.py
â”‚       â”œâ”€â”€ custom_react_parser.py
â”‚       â””â”€â”€ tokenizer.py
â”œâ”€â”€ fastapi_server
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ assistant_service.py
â”‚   â”œâ”€â”€ invoke_body.py
â”‚   â”œâ”€â”€ server_controller.py
â”‚   â””â”€â”€ server_main.py
â”œâ”€â”€ llm
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client_types
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_client_appointment.py
â”‚   â”‚   â”œâ”€â”€ llm_client_diagnosis.py
â”‚   â”‚   â”œâ”€â”€ llm_client_gemini.py
â”‚   â”‚   â”œâ”€â”€ llm_client_openai.py
â”‚   â”‚   â””â”€â”€ llm_client_topic_master.py
â”‚   â”œâ”€â”€ devices.py
â”‚   â”œâ”€â”€ llm_client.py
â”‚   â””â”€â”€ models.py
â”œâ”€â”€ main.py
â””â”€â”€ mcp_client
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ appointment_mcp.py
    â”œâ”€â”€ diagnosis_mcp.py
    â””â”€â”€ util
        â”œâ”€â”€ __init__.py
        â””â”€â”€ mcp_client.py
```

## Key Features

* **Topic Master Architecture:** A nested graph orchestrator that manages conversation state using a "Topic Stack," ensuring context retention across multi-turn dialogues.
* **Model Context Protocol (MCP):** Connects agents to external tools (Diagnosis and Appointment servers) dynamically.
* **FastAPI Streaming Server:** Supports both synchronous (`/invoke`) and WebSocket streaming (`/stream`) endpoints.
* **Model Agnostic:** Configurable to run with Google Gemini, OpenAI, or DeepInfra.

## ğŸ› ï¸ Prerequisites

* Python 3.10+
* [uv](https://docs.astral.sh/uv/) (Recommended for dependency management)

## âš™ï¸ Configuration

Create a `.env` file in the root directory. Configure the following variables based on your LLM provider and server needs:

```env
# Server Configuration
AGENTIC_SERVER_HOST=0.0.0.0
AGENTIC_SERVER_PORT=8082
AGENTIC_SERVER_API_KEY=your-secure-server-key

# Model Context Protocol (MCP) Ports
APPOINTMENT_SERVER_PORT=8081
DIAGNOSIS_SERVER_PORT=8080

# --- LLM Configurations (Example for Gemini) ---
GEMINI_API_KEY=your_google_api_key
GEMINI_API_MODEL=gemini-2.5-flash
GEMINI_API_ENDPOINT=[https://generativelanguage.googleapis.com](https://generativelanguage.googleapis.com)

# Topic Master Specific LLM (Routing Logic)
TOPIC_MASTER_LLM_TYPE=GEMINI
TOPIC_MASTER_LLM_API_KEY=your_google_api_key
TOPIC_MASTER_LLM_MODEL_NAME=gemini-2.5-flash

# Diagnosis Agent LLM
DIAGNOSIS_LLM_TYPE=GEMINI
DIAGNOSIS_LLM_API_KEY=your_google_api_key
DIAGNOSIS_LLM_MODEL_NAME=medgemma-27b # or relevant model

# Appointment Agent LLM
APPOINTMENT_LLM_TYPE=GEMINI
APPOINTMENT_LLM_API_KEY=your_google_api_key
```

## ğŸš€ Usage

### 1. Install Dependencies
```bash
uv sync
```

### 2. Run the Server
You can start the FastAPI server using the provided entry point:

```bash
uv run python -m fastapi_server.server_main
```

The server will start on `0.0.0.0:8082` (or your configured port).

### 3. API Endpoints

| Method | Endpoint | Description |
| :--- | :--- | :--- |
| `GET` | `/healthz` | Health check. |
| `POST` | `/invoke` | Single-turn invocation. Requires `thread_id` and `input` payload. |

**Example Payload (`/invoke`):**
```json
{
  "thread_id": UUID,
  "client_turn_id": UUID,
  "input": {
    "message": "I have a severe headache and need to see a doctor."
  }
}
```

